# üìÑ agent_message.py
# Agent LangGraph : r√©cup√®re les sources + g√©n√®re une r√©ponse via LLM Mistral (Ollama)

from model.retrievers.message_retriever import get_message_retriever
from langchain.prompts import PromptTemplate
from langchain_ollama import OllamaLLM

llm = OllamaLLM(model="mistral")


# üì• Chargement du prompt
with open("model/prompts/message_prompt.txt", "r") as f:
    template = f.read()

prompt = PromptTemplate(
    template=template,
    input_variables=["context", "question"]
)

# üîÅ RAG pipeline
def agent_message_node(question: str) -> str:
    print("[DEBUG] Requ√™te pos√©e √† l'agent_message :", question)

    retriever = get_message_retriever()
    docs = retriever.invoke(question)
    print(f"[DEBUG] {len(docs)} documents trouv√©s")

    context = "\n---\n".join([doc.page_content for doc in docs])
    full_prompt = prompt.format(context=context, question=question)

    response = llm.invoke(full_prompt)
    print("[‚úÖ] R√©ponse g√©n√©r√©e :")
    print(response)

    return response
