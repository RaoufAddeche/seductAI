# üìÑ agent_message.py
# Agent LangGraph : r√©cup√®re les sources + g√©n√®re une r√©ponse via LLM Mistral (Ollama)

from model.retrievers.message_retriever import get_message_retriever
from langchain.prompts import PromptTemplate
from langchain_ollama import OllamaLLM
from model.agents.llm_loader import get_llm


llm = get_llm()

# üì• Chargement du prompt
with open("model/prompts/message_prompt.txt", "r") as f:
    template = f.read()

prompt = PromptTemplate(
    template=template,
    input_variables=["context", "question"]
)

# üîÅ RAG pipeline
def agent_message_node(messages) -> str:
    """
    G√©n√®re une r√©ponse en tenant compte :
    - du thread complet (m√©moire conversationnelle)
    - des documents RAG les plus pertinents
    """
    print("[DEBUG] agent_message_node re√ßoit le contexte complet :", messages)

    # 1Ô∏è‚É£ Thread format√© (m√©moire conversation)
    thread_formatted = ""
    for m in messages:
        role = m.get("role") or m.get("sender") or "user"
        who = "Utilisateur" if role == "user" else "IA"
        thread_formatted += f"{who} : {m['content']}\n"

    # 2Ô∏è‚É£ On extrait la derni√®re question utilisateur pour le RAG
    last_user_message = next((m["content"] for m in reversed(messages) if (m.get("role") or m.get("sender")) == "user"), "")

    # 3Ô∏è‚É£ RAG ‚Äî recherche de documents pertinents sur la derni√®re question
    retriever = get_message_retriever()
    docs = retriever.invoke(last_user_message)
    print(f"[DEBUG] {len(docs)} documents RAG trouv√©s")

    # Formatage des extraits docs
    context_rag = "\n---\n".join([doc.page_content for doc in docs])

    # 4Ô∏è‚É£ Construction du prompt final
    # -> On concat√®ne les deux contextes dans 'context' pour le template
    full_context = (
        "Conversation pr√©c√©dente‚ÄØ:\n"
        f"{thread_formatted}\n"
        "Sources pertinentes‚ÄØ:\n"
        f"{context_rag}\n"
        "\nR√©ponds au dernier message utilisateur en tenant compte de la conversation et des sources ci-dessus."
    )
    question = ""  # Plus utile ici, mais conserv√© pour compatibilit√© template

    full_prompt = prompt.format(context=full_context, question=question)

    response = llm.invoke(full_prompt)
    print("[‚úÖ] R√©ponse contextuelle g√©n√©r√©e :")
    print(response.content)

    return response.content

