# üìÑ agent_classifier.py
# Ce module d√©termine dynamiquement quels agents doivent √™tre appel√©s via un prompt RAG

from langchain_ollama import OllamaLLM
import json

# üß† LLM local via Ollama (Mistral)
llm = OllamaLLM(model="mistral")

# üì• Chargement du prompt depuis le fichier
with open("model/prompts/classifier_prompt.txt", "r") as f:
    template = f.read()

def classifier_agent_node(question: str) -> list[str]:
    print("[DEBUG] Question envoy√©e au classifier :", question)

    prompt = template.format(question=question)
    response = llm.invoke(prompt)

    print("[DEBUG] R√©ponse brute du classifier :", response)

    try:
        # ‚úÖ Convertir la r√©ponse en liste Python
        agents = json.loads(response)
        print("[‚úÖ] Agents identifi√©s :", agents)
        return agents
    except json.JSONDecodeError:
        print("[‚ö†Ô∏è] Erreur : r√©ponse non JSON")
        return []
