# üìÑ score_agent.py
# √âvalue une interaction sur 4 axes : confiance, clart√©, empathie, assertivit√©

from langchain.prompts import PromptTemplate
from langchain_ollama import OllamaLLM
import json

# üîÆ LLM de scoring
llm = OllamaLLM(model="mistral")

# üìÑ Chargement du prompt
with open("model/prompts/score_prompt.txt", "r") as f:
    template = f.read()

prompt = PromptTemplate(
    template=template,
    input_variables=["question", "answer"]
)

def score_agent_node(question: str, answer: str) -> dict:
    print("[DEBUG] score_agent ‚Üí √âvaluation en cours")
    full_prompt = prompt.format(question=question, answer=answer)

    response = llm.invoke(full_prompt)
    print("[DEBUG] R√©ponse brute du LLM (scoring) :\n", response)

    try:
        scores = json.loads(response)
        assert all(k in scores for k in ["confiance", "clarte", "empathie", "assertivite"])
    except Exception as e:
        print("[‚ùå] Erreur parsing JSON :", e)
        return {
            "confiance": 0.0,
            "clarte": 0.0,
            "empathie": 0.0,
            "assertivite": 0.0
        }

    print("[‚úÖ] Scores √©valu√©s :", scores)
    return scores
